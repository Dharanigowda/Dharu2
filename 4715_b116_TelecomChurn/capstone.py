# -*- coding: utf-8 -*-
"""capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hItAcK8km_NvdSK353LHaXp5EeVos6kG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

df=pd.read_excel("/content/train.xlsx")

df.head()

df.info()

df.describe()

df.shape

df.columns

df=df.drop('customerID',axis=1)

df.isnull().sum()

# Get the numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Get the categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print('Numerical columns:', numerical_cols)
print('Categorical columns:', categorical_cols)

# replace column values with 0 and 1
df['Churn'] = df['Churn'].replace({'No': 0, 'Yes': 1})

# create a dictionary to store the mapping of labels to numerical values
label_map = {}

# iterate over the categorical columns
for col in categorical_cols:
    # get the unique values of the column
    unique_vals = df[col].unique()
    
    # create a dictionary to store the mapping of each unique value to a numerical value
    col_map = {}
    
    # assign a numerical value to each unique value
    for i, val in enumerate(unique_vals):
        col_map[val] = i
        
    # store the column mapping in the label_map dictionary
    label_map[col] = col_map
    
    # replace the categorical values in the column with the numerical values
    df[col] = df[col].map(col_map)

df.head()

print(label_map)

#checking outiliers in dataset
fig, axs = plt.subplots(ncols=4 ,nrows=5, figsize=(30, 20))
index = 0
axs = axs.flatten()
for k,v in df.items():
    sns.boxplot(y=k, data= df, ax=axs[index])
    index += 1
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

# get correlation of "Churn" with other variables
corr_matrix = df.corr()["Churn"].sort_values(ascending=False)

# plot bar chart of correlations
plt.figure(figsize=(10,5))
sns.barplot(x=corr_matrix.index, y=corr_matrix.values)
plt.xticks(rotation=90)
plt.xlabel("Variables")
plt.ylabel("Correlation with Churn")
plt.show()

# create a correlation matrix
corr_matrix = df.corr()

# plot the heatmap
plt.figure(figsize=(20, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation of Churn with other variables')
plt.show()

sns.countplot(x='gender', data=df)

# Get the count of senior citizens
senior_count = df[df['SeniorCitizen'] == 1]['SeniorCitizen'].count()

# Get the count of non-senior citizens
non_senior_count = df[df['SeniorCitizen'] == 0]['SeniorCitizen'].count()

# Create a list of values
values = [senior_count, non_senior_count]

# Create a list of labels
labels = ['SeniorCitizens', 'Non-SeniorCitizens']

# Set the explode parameter to highlight the Senior Citizens slice
explode = (0.1, 0)

# Create a pie chart
plt.pie(values, labels=labels, explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)

# Add a title to the chart
plt.title('Senior Citizen Distribution')

# Show the chart
plt.show()

# count the number of customers by account type
account_counts = df['tenure'].value_counts()

# create a bar chart of account type counts
plt.bar(account_counts.index, account_counts.values)

# set the title and labels for the chart
plt.title('Customer Account Information')
plt.xlabel('Tenure')
plt.ylabel('Number of Customers')

# display the chart
plt.show()

ax = df['Contract'].value_counts().plot(kind = 'bar',rot = 0, width = 0.3)
ax.set_ylabel('of Customers')
ax.set_title('of Customers by Contract Type')

services = ['PhoneService','MultipleLines','InternetService','OnlineSecurity',
           'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']

fig, axes = plt.subplots(nrows = 3,ncols = 3,figsize = (15,12))
for i, item in enumerate(services):
    if i < 3:
        ax = df[item].value_counts().plot(kind = 'bar',ax=axes[i,0],rot = 0)
        
    elif i >=3 and i < 6:
        ax = df[item].value_counts().plot(kind = 'bar',ax=axes[i-3,1],rot = 0)
        
    elif i < 9:
        ax = df[item].value_counts().plot(kind = 'bar',ax=axes[i-6,2],rot = 0)
    ax.set_title(item)

df[['MonthlyCharges', 'TotalCharges']].plot.scatter(x = 'MonthlyCharges',
                                                              y='TotalCharges')

ax = (df['Churn'].value_counts()*100.0 /len(df)).plot(kind='bar',figsize = (8,6))
ax.set_ylabel(' Customers',size = 14)
ax.set_xlabel('Churn',size = 14)
ax.set_title('Churn Rate', size = 14)

sns.boxplot(x = df.Churn, y = df.tenure)

contract_churn = df.groupby(['Contract','Churn']).size().unstack()

ax = (contract_churn.T*100.0 / contract_churn.T.sum()).T.plot(kind='bar')
ax.legend(loc='best',prop={'size':14},title = 'Churn')
ax.set_ylabel(' Customers',size = 14)
ax.set_title('Churn by Contract Type',size = 14)

seniority_churn = df.groupby(['SeniorCitizen','Churn']).size().unstack()

ax = (seniority_churn.T*100.0 / seniority_churn.T.sum()).T.plot(kind='bar',
                                                                width = 0.2,
                                                                stacked = True,
                                                                rot = 0, 
                                                                figsize = (8,6))
ax.legend(loc='center',prop={'size':14},title = 'Churn')
ax.set_ylabel('Customers')
ax.set_title('Churn by Seniority Level',size = 14)

y = df['Churn'].values
X = df.drop(columns = ['Churn'])

from imblearn.over_sampling import SMOTE

# Apply SMOTE to target variable only
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Convert y_resampled to pandas dataframe
y_resampled = pd.DataFrame(y_resampled, columns=['Churn'])

ax = (y_resampled['Churn'].value_counts()*100.0 /len(y_resampled)).plot(kind='bar',figsize = (8,6))
ax.set_ylabel(' Customers',size = 14)
ax.set_xlabel('Churn',size = 14)
ax.set_title('Churn Rate', size = 14)

# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

#predicting the model
y_pred = model.predict(X_test)

print(y_pred)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

from sklearn.ensemble import RandomForestClassifier
# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)
model_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,
                                  random_state =50, max_features = "auto",
                                  max_leaf_nodes = 30)
model_rf.fit(X_train, y_train)

from sklearn import metrics

# Make predictions
prediction_test = model_rf.predict(X_test)
print (metrics.accuracy_score(y_test, prediction_test))

# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

from sklearn.svm import SVC

model.svm = SVC(kernel='linear') 
model.svm.fit(X_train,y_train)
preds = model.svm.predict(X_test)
metrics.accuracy_score(y_test, preds)

# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

from sklearn.tree import DecisionTreeClassifier

# Train decision tree model
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Test decision tree model
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

from xgboost import XGBClassifier
xgb_model = XGBClassifier(colsample_bytree= 0.9071168163710651, 
                      gamma= 0.6400419533775026, 
                      max_depth= round(6.112471405351496), 
                      min_child_weight= 4.025267705883393, 
                      subsample= 1.0)
xgb_model.fit(X_train, y_train)
preds = xgb_model.predict(X_test)
metrics.accuracy_score(y_test, preds)

import xgboost as xgb
from sklearn.model_selection import cross_val_score
from bayes_opt import BayesianOptimization

# Define the objective function for hyperparameter tuning
def xgb_cv(max_depth, gamma, min_child_weight, subsample, colsample_bytree):
    # Define the XGBoost classifier with the specified hyperparameters
    xgb_model = xgb.XGBClassifier(
        max_depth=int(max_depth),
        gamma=gamma,
        min_child_weight=min_child_weight,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        objective='binary:logistic',
        eval_metric='auc',
        random_state=42
    )
    # Evaluate the classifier using cross-validation
    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='roc_auc')
    # Return the mean AUC score across cross-validation folds
    return scores.mean()

# Define the bounds for the hyperparameters to search over
pbounds = {
    'max_depth': (3, 7),
    'gamma': (0, 1),
    'min_child_weight': (1, 10),
    'subsample': (0.5, 1),
    'colsample_bytree': (0.5, 1)
}

# Initialize the Bayesian Optimization object with the objective function and bounds
xgb_bo = BayesianOptimization(
    f=xgb_cv,
    pbounds=pbounds,
    random_state=42
)

# Run the optimization for the specified number of iterations
xgb_bo.maximize(init_points=10, n_iter=20, acq='ei')

# Print the best hyperparameters and corresponding AUC score
print(xgb_bo.max)

pip install bayesian-optimization

import pickle

# Save the model as a .pkl file
filename = 'xgb_model.pkl'
with open(filename, 'wb') as file:
    pickle.dump(xgb_model, file)

test=pd.read_excel("/content/test1.xlsx")

prediction=xgb_model.predict(test)

prediction

